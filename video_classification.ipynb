{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.712727Z",
     "start_time": "2024-11-06T14:39:54.711071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "# Suppress all SyntaxWarning warnings\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"pytorchvideo\")"
   ],
   "id": "88c08279b3594a31",
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.729676Z",
     "start_time": "2024-11-06T14:39:54.728131Z"
    }
   },
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load file names into dataframe",
   "id": "387526c0b986d8ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "non = glob(\"NonViolence/*\")\n",
    "vio = glob(\"Violence/*\")\n",
    "label = [0] * len(non) + [1] * len(vio)\n",
    "\n",
    "non_full_paths = [os.path.abspath(file) for file in non]\n",
    "vio_full_paths = [os.path.abspath(file) for file in vio]\n",
    "\n",
    "df = pd.DataFrame(zip(non_full_paths + vio_full_paths, label), columns=[\"file\", \"label\"])\n",
    "print(\"non violence video\", len(non))\n",
    "print(\"violence video\", len(vio))\n",
    "df.head(10)"
   ],
   "id": "d85825bbd7e26f9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.7 * len(df))  # 70%\n",
    "val_size = int(0.15 * len(df))    # 15%\n",
    "test_size = len(df) - train_size - val_size  # Remaining 15%\n",
    "\n",
    "train_df, val_df, test_df = random_split(df, [train_size, val_size, test_size])\n",
    "# train_df, val_df = train_test_split(df, test_size=0.2, shuffle=True) # random_state=42,\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n"
   ],
   "id": "1d30509f9955af88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.880410Z",
     "start_time": "2024-11-06T14:39:54.879148Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "52bc2cea6dc6132c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_video_paths = [\n",
    "    (row['file'], {'label': row['label']}) for _, row in train_df.iterrows()\n",
    "] # required by pytorchvideo: list of tuples with file path and a dictionary with label\n",
    "print(\"number of train:\", len(train_video_paths))\n",
    "train_video_paths[0]"
   ],
   "id": "c4bc8eb858dbe4ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cbe9c42ab999c9a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_video_paths = [\n",
    "    (row['file'], {'label': row['label']}) for _, row in val_df.iterrows()\n",
    "]\n",
    "print(\"number of val:\", len(val_video_paths))\n",
    "val_video_paths[0]"
   ],
   "id": "85d8c02d035e1cbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Augmentation",
   "id": "4babcf02d48fd99a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pytorchvideo.data import labeled_video_dataset, kinetics, make_clip_sampler, LabeledVideoDataset\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "\n",
    "    UniformTemporalSubsample,\n",
    "    Permute,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    CenterCrop,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo, \n",
    "    NormalizeVideo,\n",
    ")\n",
    "\n",
    "# Could try to replace _trainsforms_video with functional API\n",
    "# import torch\n",
    "# import torchvision.transforms.functional as F\n",
    "# \n",
    "# # Define the transformation functions using functional API\n",
    "# def center_crop_video(video, output_size):\n",
    "#     # video is a tensor of shape (C, T, H, W)\n",
    "#     c, t, h, w = video.shape\n",
    "#     cropped_frames = [F.center_crop(video[:, i, :, :], output_size) for i in range(t)]\n",
    "#     return torch.stack(cropped_frames, dim=1)  # (C, T, H', W')\n",
    "# \n",
    "# def normalize_video(video, mean, std):\n",
    "#     # video is a tensor of shape (C, T, H, W)\n",
    "#     c, t, h, w = video.shape\n",
    "#     normalized_frames = [F.normalize(video[:, i, :, :], mean, std) for i in range(t)]\n",
    "#     return torch.stack(normalized_frames, dim=1)  # (C, T, H, W)\n",
    "# \n",
    "# # Example usage\n",
    "# video = torch.randn(3, 10, 128, 128)  # Example video tensor (C, T, H, W)\n",
    "# output_size = (100, 100)\n",
    "# mean = [0.5, 0.5, 0.5]\n",
    "# std = [0.5, 0.5, 0.5]\n",
    "# \n",
    "# cropped_video = center_crop_video(video, output_size)\n",
    "# normalized_video = normalize_video(cropped_video, mean, std)\n",
    "# \n",
    "# print(normalized_video.shape)  # Should be (3, 10, 100, 100)\n"
   ],
   "id": "8226bae5e3a6aa42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.951952254Z",
     "start_time": "2024-11-06T14:25:35.255702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key=\"video\", # ensures that the specified sequence of transformations (wrapped in Compose) is applied to the video data only\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(20), # Reduces the number of frames in the video to 20 by sampling frames uniformly across the entire video\n",
    "            Lambda(lambda x: x / 255.0), # Normalizes the pixel values by range scaling to [0, 1]\n",
    "            Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)), # Standardizes the pixel values to ensure mean=0 and std=1\n",
    "            RandomShortSideScale(min_size=248, max_size=256), # Scales the short side of each frame randomly between 248 and 256 pixels\n",
    "            CenterCropVideo(224), # Crops the center of each frame to a size of 224x224 pixels\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "    ])),\n",
    "])"
   ],
   "id": "699a622022b5d125",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_video_paths",
   "id": "536e78f173d1b0b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.952375025Z",
     "start_time": "2024-11-06T14:25:35.315327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import dataloader\n",
    "# train_dataset = LabeledVideoDataset(train_video_paths, \n",
    "#                                     clip_sampler=make_clip_sampler(\"random\", 2), # video duration is 2 seconds\n",
    "#                                     transform=video_transform,\n",
    "#                                     decode_audio=False) #  \n",
    "# loader = dataloader.DataLoader(train_dataset, batch_size=4, num_workers=0, pin_memory=False) # collate_fn=collate_fn"
   ],
   "id": "b8577475819ee765",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.952488948Z",
     "start_time": "2024-11-06T14:25:35.361416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try to load a batch and catch exceptions\n",
    "# try:\n",
    "#     batch = next(iter(loader))\n",
    "#     print(batch)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading video: {e}\")"
   ],
   "id": "3a4f1aeb21757d1a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect the dataloader",
   "id": "4137a6ff7eef0ba9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.952557165Z",
     "start_time": "2024-11-06T14:25:35.406102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Inspect the batch structure\n",
    "# print(\"Batch keys:\", batch.keys())\n",
    "# \n",
    "# # Assuming the batch contains 'video' and 'label'\n",
    "# # Inspect the shape of the 'video' tensor\n",
    "# video_data = batch['video']\n",
    "# print(\"Video data shape:\", video_data.shape) # (batch size, channels, frames, height, width)\n",
    "# \n",
    "# # Inspect the shape of the 'label' tensor\n",
    "# label_data = batch['label']\n",
    "# print(\"Label data shape:\", label_data.shape)"
   ],
   "id": "926c360c4b034ad1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect the dataset before setting into dataloader",
   "id": "ba7fd0e05013c8f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.952636774Z",
     "start_time": "2024-11-06T14:25:35.450769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# try:\n",
    "#     for i in range(1):  # Inspect first item\n",
    "#         sample = next(iter(train_dataset))\n",
    "#         print(f\"Sample {i}:\")\n",
    "#         for key, value in sample.items():\n",
    "#             print(f\"  {key}: type={type(value)}, shape={value.shape if hasattr(value, 'shape') else 'N/A'}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error accessing sample: {e}\")"
   ],
   "id": "935ee4887692bb9a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model architecture",
   "id": "6ca9d1a9df044ea4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.952715983Z",
     "start_time": "2024-11-06T14:25:35.494927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics"
   ],
   "id": "5b810191cc976d28",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "video_model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"efficient_x3d_xs\", pretrained=True)\n",
    "video_model"
   ],
   "id": "756c0452e32f37cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.967088836Z",
     "start_time": "2024-11-06T14:25:36.719737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class ViolenceClassifier(LightningModule):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(ViolenceClassifier, self).__init__()\n",
    "        self.video_model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"efficient_x3d_xs\", pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(400, 1) #  efficient_x3d_xs model has 400 output features\n",
    "\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 4\n",
    "        self.num_worker = 4\n",
    "\n",
    "        # Evaluation metric\n",
    "        self.metric = torchmetrics.Accuracy(task='binary')\n",
    "\n",
    "        # Loss function \n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Initialize lists to store outputs\n",
    "        self.training_step_outputs = []  # Initialize empty list for training outputs\n",
    "        self.validation_step_outputs = []  # Initialize empty list for validation outputs\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.video_model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = LabeledVideoDataset(\n",
    "            train_video_paths,\n",
    "            clip_sampler=make_clip_sampler(\"random\", 2), # Video duration is 2 seconds\n",
    "            transform=video_transform,\n",
    "            decode_audio=False\n",
    "        )\n",
    "        # that number of worker processes are spawned and used to load the data. \n",
    "        # Each worker will fetch a batch of data from the dataset independently. \n",
    "        # The fetched data is then collated into a batch and returned by the DataLoader.\n",
    "        # pin_memory: If True, the data loader will copy Tensors into CUDA pinned memory before returning them. \n",
    "        # This can make data transfer to the GPU faster. \n",
    "        # This is generally set to True when the model is being trained on a GPU.\n",
    "        loader = DataLoader(train_dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=False) \n",
    "        return loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        output = self(video) # Same as self.forward(x)\n",
    "        label = label.unsqueeze(1)  # Reshape to (batch_size, 1)\n",
    "        loss = self.criterion(output, label.to(torch.float32))\n",
    "        metric = self.metric(output, label.int())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_metric\", metric, prog_bar=True)\n",
    "        # Store loss and metric for later use in epoch end\n",
    "        self.training_step_outputs.append({\"loss\": loss, \"metric\": metric})\n",
    "        return {\"loss\": loss, \"metric\": metric.detach()}\n",
    "        \n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # This is called at the end of the training epoch\n",
    "        # Calculate average loss and metric\n",
    "        avg_loss = torch.stack([x['loss'].detach() for x in self.training_step_outputs]).mean().cpu().numpy().round(2)\n",
    "        avg_metric = torch.stack([x['metric'].detach() for x in self.training_step_outputs]).mean().cpu().numpy().round(2)\n",
    "        \n",
    "        self.log(\"epoch\", self.current_epoch, prog_bar=True, logger=True)\n",
    "        self.log(\"avg_train_loss\", avg_loss, prog_bar=True)\n",
    "        self.log(\"avg_train_metric\", avg_metric, prog_bar=True)\n",
    "        # Clear the stored outputs\n",
    "        self.training_step_outputs.clear()\n",
    "        \n",
    "    # def training_epoch_end(self, outputs):\n",
    "    #     avg_loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "    #     metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "    #     self.log(\"train_loss\", avg_loss)\n",
    "    #     self.log(\"train_metric\", metric)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = LabeledVideoDataset(\n",
    "            val_video_paths,\n",
    "            clip_sampler=make_clip_sampler(\"random\", 2), # Video duration is 2 seconds\n",
    "            transform=video_transform,\n",
    "            decode_audio=False\n",
    "        )\n",
    "        loader = DataLoader(val_dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=False)\n",
    "        return loader\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        output = self(video) # Same as self.forward(x)\n",
    "        label = label.unsqueeze(1)  # Reshape to (batch_size, 1)\n",
    "        loss = self.criterion(output, label.to(torch.float32))\n",
    "        metric = self.metric(output, label.int())\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_metric\", metric, prog_bar=True)\n",
    "        # Store loss and metric for later use in epoch end\n",
    "        self.validation_step_outputs.append({\"loss\": loss, \"metric\": metric})\n",
    "        return {\"loss\": loss, \"metric\": metric.detach()}\n",
    "\n",
    "    def  on_validation_epoch_end(self):\n",
    "        # Calculate average loss and metric for the validation epoch\n",
    "        avg_loss = torch.stack([x['loss'].detach()  for x in self.validation_step_outputs]).mean().cpu().numpy().round(2)\n",
    "        avg_metric = torch.stack([x['metric'].detach()  for x in self.validation_step_outputs]).mean().cpu().numpy().round(2)\n",
    "\n",
    "        self.log(\"epoch\", self.current_epoch, prog_bar=True, logger=True)\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        self.log(\"avg_val_metric\", avg_metric)\n",
    "        # Clear the stored outputs\n",
    "        self.validation_step_outputs.clear()\n",
    "        \n",
    "        # avg_loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        # metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        # self.log(\"val_loss\", avg_loss)\n",
    "        # self.log(\"val_metric\", metric)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = LabeledVideoDataset(\n",
    "            val_video_paths,\n",
    "            clip_sampler=make_clip_sampler(\"random\", 2), # Video duration is 2 seconds\n",
    "            transform=video_transform,\n",
    "            decode_audio=False\n",
    "        )\n",
    "        loader = DataLoader(test_dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=False)\n",
    "        return loader\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        output = self(video) # Same as self.forward(x)\n",
    "        return {\"label\": label.detach(), \"pred\": output.detach()}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        labels = torch.cat([x['label'].detach()  for x in outputs]).cpu().numpy()\n",
    "        preds = torch.cat([x['pred'].detach()  for x in outputs]).cpu().numpy()\n",
    "        preds = np.where(preds > 0.5, 1, 0)\n",
    "\n",
    "        self.log(\"epoch\", self.current_epoch, prog_bar=True, logger=True)\n",
    "        print(classification_report(labels, preds))\n",
    "        "
   ],
   "id": "430230d1fac6fa91",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:39:54.967309929Z",
     "start_time": "2024-07-30T23:34:24.801620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath='checkpoints',filename='file', save_last=True) # True means save the last model; False means when the model stopped improving\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')"
   ],
   "id": "15eb69d36a070f3c",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "model=ViolenceClassifier(num_classes=1)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed_everything(42)\n",
    "\n",
    "# Define the trainer with the appropriate precision setting\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='gpu',\n",
    "    devices=-1,\n",
    "    log_every_n_steps=1,  # Log metrics after every batch\n",
    "    #precision=\"16-mixed\",  # Enable mixed precision training\n",
    "    accumulate_grad_batches=1, # 2,\n",
    "    enable_progress_bar=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[LearningRateMonitor(logging_interval='step'), ModelCheckpoint()],\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    ")\n",
    "\n",
    "\n",
    "# model=ViolenceClassifier(num_classes=1)\n",
    "# seed_everything(42)\n",
    "# trainer = Trainer(max_epochs=1, accelerator='gpu', devices=-1,\n",
    "#                   precision=16,\n",
    "#                   accumulate_grad_batches=2,\n",
    "#                   enable_progress_bar=False,\n",
    "#                   num_sanity_val_steps=0,\n",
    "#                   callbacks=[lr_monitor, checkpoint_callback],\n",
    "# )\n"
   ],
   "id": "24ed50e7d5bbeee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.fit(model)",
   "id": "dd332ff1c08f0b26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_results = trainer.validate(model)\n",
    "print(val_results)"
   ],
   "id": "70d62f982a0c2f9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:47:20.448283Z",
     "start_time": "2024-11-06T14:47:19.439877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ],
   "id": "33636c620fc9aa92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T23:54:59.508891Z",
     "start_time": "2024-07-30T23:54:59.231451Z"
    }
   },
   "cell_type": "code",
   "source": "!kill 91443",
   "id": "f24721fd735a7059",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8fd62e8219bd6a83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eb740710b26ab415"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
